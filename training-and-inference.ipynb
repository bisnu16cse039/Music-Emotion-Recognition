{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8a7f98",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-17T11:42:30.438827Z",
     "iopub.status.busy": "2025-01-17T11:42:30.438541Z",
     "iopub.status.idle": "2025-01-17T11:42:31.165825Z",
     "shell.execute_reply": "2025-01-17T11:42:31.164791Z"
    },
    "papermill": {
     "duration": 0.731838,
     "end_time": "2025-01-17T11:42:31.167315",
     "exception": false,
     "start_time": "2025-01-17T11:42:30.435477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: ['song_id', ' valence_mean', ' valence_std', ' arousal_mean', ' arousal_std']\n",
      "Cleaned columns: ['song_id', 'valence_mean', 'valence_std', 'arousal_mean', 'arousal_std']\n",
      "Saved cleaned data to clean_train_labels.csv\n",
      "Original columns: ['song_id', ' valence_mean', ' valence_std', ' valence_ max_mean', ' valence_max_std', ' valence_min_mean', ' valence_min_std', ' arousal_mean', ' arousal_std', ' arousal_max_mean', ' arousal_max_std', ' arousal_min_mean', ' arousal_min_std']\n",
      "Cleaned columns: ['song_id', 'valence_mean', 'valence_std', 'valence__max_mean', 'valence_max_std', 'valence_min_mean', 'valence_min_std', 'arousal_mean', 'arousal_std', 'arousal_max_mean', 'arousal_max_std', 'arousal_min_mean', 'arousal_min_std']\n",
      "Saved cleaned data to clean_test_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fix_column_names(label_path, save_path=None):\n",
    "    \"\"\"\n",
    "    Fix column names in the label file by standardizing them.\n",
    "    \n",
    "    Args:\n",
    "        label_path (str): Path to the original label CSV file\n",
    "        save_path (str, optional): Path to save the cleaned CSV file. \n",
    "                                 If None, will return the DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame if save_path is None, else None\n",
    "    \"\"\"\n",
    "    # Read the label file\n",
    "    df = pd.read_csv(label_path)\n",
    "    \n",
    "    # Print original columns\n",
    "    print(\"Original columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Clean column names:\n",
    "    # 1. Convert to lowercase\n",
    "    # 2. Remove extra spaces\n",
    "    # 3. Replace spaces with underscores\n",
    "    df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "    \n",
    "    # Map variations to standard names\n",
    "    column_mapping = {\n",
    "        'songid': 'song_id',\n",
    "        'song_id': 'song_id',\n",
    "        'valencemean': 'valence_mean',\n",
    "        'valence_mean': 'valence_mean',\n",
    "        'valencestd': 'valence_std',\n",
    "        'valence_std': 'valence_std',\n",
    "        'arousalmean': 'arousal_mean',\n",
    "        'arousal_mean': 'arousal_mean',\n",
    "        'arousalstd': 'arousal_std',\n",
    "        'arousal_std': 'arousal_std'\n",
    "    }\n",
    "    \n",
    "    # Rename columns based on mapping\n",
    "    #df = df.rename(columns=column_mapping)\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    \n",
    "    # Print new columns\n",
    "    print(\"Cleaned columns:\", df.columns.tolist())\n",
    "    \n",
    "    # Check if all required columns are present\n",
    "    required_columns = ['song_id', 'valence_mean', 'valence_std', 'arousal_mean', 'arousal_std']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    if save_path:\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Saved cleaned data to {save_path}\")\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Fix training labels\n",
    "    train_label_path = \"/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_Annotations/annotations/annotations averaged per song/song_level/static_annotations_averaged_songs_1_2000.csv\"\n",
    "    test_label_path = \"/kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_Annotations/annotations/annotations averaged per song/song_level/static_annotations_averaged_songs_2000_2058.csv\"\n",
    "    \n",
    "    # Fix and save training labels\n",
    "    fix_column_names(\n",
    "        train_label_path, \n",
    "        save_path=\"clean_train_labels.csv\"\n",
    "    )\n",
    "    \n",
    "    # Fix and save test labels\n",
    "    fix_column_names(\n",
    "        test_label_path, \n",
    "        save_path=\"clean_test_labels.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8bbbf1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-17T11:42:31.171678Z",
     "iopub.status.busy": "2025-01-17T11:42:31.171431Z",
     "iopub.status.idle": "2025-01-17T11:43:09.627413Z",
     "shell.execute_reply": "2025-01-17T11:43:09.626430Z"
    },
    "papermill": {
     "duration": 38.459544,
     "end_time": "2025-01-17T11:43:09.628779",
     "exception": false,
     "start_time": "2025-01-17T11:42:31.169235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1744 songs with both features and labels\n",
      "Loaded 1744 songs with both features and labels\n",
      "Using device: cuda\n",
      "Epoch [10/100], Train Loss: 0.8296, Val Loss: 0.4915\n",
      "Epoch [20/100], Train Loss: 0.7114, Val Loss: 0.4791\n",
      "Epoch [30/100], Train Loss: 0.7103, Val Loss: 0.4737\n",
      "Epoch [40/100], Train Loss: 0.6568, Val Loss: 0.4904\n",
      "Epoch [50/100], Train Loss: 0.6428, Val Loss: 0.4657\n",
      "Epoch [60/100], Train Loss: 0.6221, Val Loss: 0.4689\n",
      "Epoch [70/100], Train Loss: 0.6175, Val Loss: 0.4637\n",
      "Epoch [80/100], Train Loss: 0.5812, Val Loss: 0.4563\n",
      "Epoch [90/100], Train Loss: 0.5870, Val Loss: 0.4722\n",
      "Epoch [100/100], Train Loss: 0.5623, Val Loss: 0.4568\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class AudioEmotionDataset(Dataset):\n",
    "    def __init__(self, feature_dir, label_path, scaler=None, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_dir (str): Single directory containing all feature CSV files\n",
    "            label_path (str): Path to label CSV file (train or test)\n",
    "            scaler (StandardScaler, optional): Scaler for feature normalization\n",
    "            train (bool): If True, fit_transform scaler; if False, transform only\n",
    "        \"\"\"\n",
    "        self.feature_dir = feature_dir\n",
    "        self.label_df = pd.read_csv(label_path)\n",
    "        self.song_ids = self.label_df['song_id'].values\n",
    "        self.train = train\n",
    "        \n",
    "        # Load and process features for songs that have labels\n",
    "        self.features = []\n",
    "        self.valid_indices = []  # Keep track of songs that have both features and labels\n",
    "        self.valid_song_ids = []\n",
    "        \n",
    "        for idx, song_id in enumerate(self.song_ids):\n",
    "            feature_path = os.path.join(feature_dir, f\"{song_id}_features.csv\")\n",
    "            if os.path.exists(feature_path):\n",
    "                try:\n",
    "                    song_features = pd.read_csv(feature_path)\n",
    "                    # Remove end_time column\n",
    "                    song_features = song_features.drop('end_time', axis=1)\n",
    "                    # Calculate mean of features across all segments\n",
    "                    mean_features = song_features.mean().values\n",
    "                    self.features.append(mean_features)\n",
    "                    self.valid_indices.append(idx)\n",
    "                    self.valid_song_ids.append(song_id)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {song_id}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        self.features = np.array(self.features)\n",
    "        \n",
    "        # Keep only labels for songs that have features\n",
    "        self.label_df = self.label_df.iloc[self.valid_indices]\n",
    "        \n",
    "        # Scale features\n",
    "        if scaler is None and train:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.features = self.scaler.fit_transform(self.features)\n",
    "        elif scaler is not None and not train:\n",
    "            self.scaler = scaler\n",
    "            self.features = self.scaler.transform(self.features)\n",
    "        \n",
    "        # Prepare labels\n",
    "        self.labels = self.label_df[['valence_mean', 'valence_std', \n",
    "                                   'arousal_mean', 'arousal_std']].values\n",
    "        \n",
    "        print(f\"Loaded {len(self.features)} songs with both features and labels\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.FloatTensor(self.features[idx])\n",
    "        labels = torch.FloatTensor(self.labels[idx])\n",
    "        return features, labels\n",
    "\n",
    "class EmotionPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EmotionPredictor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 4)  # 4 outputs: valence_mean, valence_std, arousal_mean, arousal_std\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_model(train_loader, val_loader, model, num_epochs=100, learning_rate=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict().copy()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Set paths\n",
    "    feature_dir = \"/kaggle/input/deam-featuresmfcc-rms\"  # Single directory containing all features\n",
    "    train_labels_path = \"/kaggle/working/clean_train_labels.csv\"\n",
    "    test_labels_path = \"/kaggle/working/clean_train_labels.csv\"\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = AudioEmotionDataset(feature_dir, train_labels_path, train=True)\n",
    "    test_dataset = AudioEmotionDataset(feature_dir, test_labels_path, \n",
    "                                     scaler=train_dataset.scaler, train=False)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = train_dataset.features.shape[1]  # Number of features\n",
    "    model = EmotionPredictor(input_size)\n",
    "    \n",
    "    # Train model\n",
    "    best_model_state = train_model(train_loader, test_loader, model)\n",
    "    \n",
    "    # Save best model\n",
    "    torch.save(best_model_state, 'best_emotion_model.pth')\n",
    "    \n",
    "    # Save scaler for future use\n",
    "    import joblib\n",
    "    joblib.dump(train_dataset.scaler, 'feature_scaler.pkl')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2dd200",
   "metadata": {
    "papermill": {
     "duration": 0.001871,
     "end_time": "2025-01-17T11:43:09.634148",
     "exception": false,
     "start_time": "2025-01-17T11:43:09.632277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1427290,
     "sourceId": 2362956,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6498874,
     "sourceId": 10496389,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43.300047,
   "end_time": "2025-01-17T11:43:11.156748",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-17T11:42:27.856701",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
